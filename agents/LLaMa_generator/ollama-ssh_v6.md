
# Ollama-SSH: Генерация текстового контента

Этот скрипт предназначен для интерактивной генерации текстового контента с использованием моделей Ollama. Он объединяет выбор модели, файлов сообщений и файла для записи ответов в единый удобный интерфейс с гибким управлением процессом генерации.

## Как работает скрипт

Скрипт последовательно выполняет следующие шаги:

1. **Выбор конфигурации**  
   При запуске, если конфигурационные переменные не заданы, скрипт сразу предлагает выбрать:
   - Модель для генерации (через вызов API Ollama для получения списка доступных моделей).
   - Файл с сообщениями (например, `01_user_message.txt`).
   - Файл для записи ответов (например, `04_model_response.txt`).

2. **Формирование запроса для генерации**  
   Для каждой строки из файла `01_user_message.txt` скрипт:
   - Считывает строку сообщения.
   - Добавляет содержимое файла `02_sys_prompt.txt` (системное задание).
   - Добавляет содержимое файла `03_response_template.txt` (шаблон ответа).
   
   Таким образом, окончательный запрос формируется путем объединения этих трёх частей в заданной последовательности.

3. **Генерация и запись ответа**  
   - Сформированный запрос отправляется выбранной модели Ollama.
   - Ответ модели выводится в консоль и дописывается в файл `04_model_response.txt`.
   - Скрипт обрабатывает сообщения последовательно, сохраняя номер последней обработанной строки для возможности продолжения с того же места в случае прерывания.

4. **Интерактивное управление**  
   - Пользователь может выбирать номер строки для начала генерации, что позволяет продолжать работу после прерываний.
   - При нажатии `CTRL+C` скрипт не завершается, а приостанавливается с сохранением последней обработанной строки и возвращается в главное меню.
   - В меню также предусмотрена опция перезапуска Ollama через отдельный скрипт `restart_ollama.py`.

## Применение скрипта

Скрипт может быть использован для:
- **Автоматизированной генерации контента:**  
  Помогает генерировать текстовые ответы, новости, статьи или сценарии, комбинируя ввод пользователя с предопределёнными подсказками и шаблонами.
  
- **Поддержки интерактивного диалога:**  
  Может быть применён в чат-ботах или системах автоматического ответа, где требуется динамически генерировать текстовые сообщения.

- **Обучения и прототипирования:**  
  Подходит для экспериментов с генеративными моделями, позволяя тестировать различные конфигурации и шаблоны ввода для получения разнообразных результатов.

- **Автоматизации рабочих процессов:**  
  Может интегрироваться в системы автоматизации для создания отчетов или других документов на основе шаблонов и пользовательских данных.

## Требования

- **Python 3.6+**
- Библиотека `ollama` (устанавливается через `pip install -r requirements.txt`)
- Стандартные библиотеки: `time`, `subprocess`, `os`, `glob`, `sys`, `signal`, `datetime`, `traceback`

## Установка и запуск

1. **Клонируйте репозиторий:**
   ```bash
   git clone <URL_вашего_репозитория>
   cd <папка_репозитория>
   ```

2. **Установите зависимости:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Запустите скрипт:**
   ```bash
   python <имя_скрипта>.py
   ```

При запуске скрипт проверит, что все необходимые переменные заданы. Если нет — он сразу предложит выбрать модель, файл с сообщениями и файл для записи ответов, после чего вы попадёте в главное меню для дальнейшей работы.

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности см. в файле [LICENSE](LICENSE).

## Контакты

Если у вас есть вопросы или предложения, создайте issue в репозитории или свяжитесь по [email](mailto:your.email@example.com).
```
