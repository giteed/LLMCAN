
# Ollama-SSH: Генерация текстового контента

Этот скрипт предназначен для интерактивной генерации текстового контента с использованием моделей Ollama. Он объединяет выбор модели, файлов сообщений и файла для записи ответов в единый удобный интерфейс с гибким управлением процессом генерации.

## Как работает скрипт

Скрипт последовательно выполняет следующие шаги:


```plaintext
          +---------------------------+
          |       Запуск скрипта      |
          +---------------------------+
                      │
                      ▼
          +---------------------------+
          | Проверка переменных:      |
          | MODEL, USER_MESSAGE_FILE, |
          | RESPONSE_FILE             |
          +---------------------------+
                      │
              ┌───────┴─────────┐
              │                 │
              ▼                 ▼
+----------------------+  +----------------------+
| Если переменные не   |  | Если все переменные  |
| заполнены, то:       |  | заполнены            |
| - Выбор модели       |  +----------------------+
| - Выбор файла        |             │
|   сообщений          |             ▼
| - Выбор файла для    |  +---------------------------+
|   записи ответов     |  | Переход в главное меню:   |
+----------------------+  | вывод текущей конфигурации|
                          | (модель, файлы, номера     |
                          | строк) и опций меню        |
                          +---------------------------+
                                     │
                                     ▼
                         +------------------------------+
                         | Главное меню:                |
                         | 1 - Начать генерацию         |
                         | 2 - Выбрать модель           |
                         | 3 - Выбрать файлы            |
                         | 4 - Перезапустить Ollama     |
                         | 5 - Выход                    |
                         +------------------------------+
                                     │
                     ┌───────────────┴───────────────┐
                     │                               │
                     ▼                               ▼
          +-------------------+           +-------------------+
          | 1 - Начать        |           | 2/3 - Выбор       |
          | генерацию         |           | модели/файлов     |
          +-------------------+           +-------------------+
                     │                               │
                     ▼                               ▼
          +---------------------------+    (Обновление переменных)
          | Ввод номера строки для    |
          | начала генерации (или "q"  |
          | для возврата в меню)      |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Чтение файла 01_user_     |
          | message.txt построчно     |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Формирование запроса:     |
          | 1. Добавление содержимого |
          |    02_sys_prompt.txt      |
          | 2. Добавление содержимого |
          |    03_response_template.txt|
          | 3. Добавление строки из    |
          |    01_user_message.txt    |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Отправка запроса в модель |
          | Ollama и получение ответа |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Запись ответа в файл      |
          | 04_model_response.txt      |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Обновление LAST_PROCESSED  |
          | _LINE после каждой строки  |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Обработка прерывания (CTRL+C)|
          | - Сохранение состояния     |
          | - Возврат в главное меню   |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Опция перезапуска Ollama:  |
          | Запуск restart_ollama.py   |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          |       Выход из скрипта    |
          +---------------------------+
```


1. **Выбор конфигурации**  
   При запуске, если конфигурационные переменные не заданы, скрипт сразу предлагает выбрать:
   - Модель для генерации (через вызов API Ollama для получения списка доступных моделей).
   - Файл с сообщениями (например, `01_user_message.txt`).
   - Файл для записи ответов (например, `04_model_response.txt`).

2. **Формирование запроса для генерации**  
   Для каждой строки из файла `01_user_message.txt` скрипт:
   - Считывает строку сообщения.
   - Добавляет содержимое файла `02_sys_prompt.txt` (системное задание).
   - Добавляет содержимое файла `03_response_template.txt` (шаблон ответа).
   
   Таким образом, окончательный запрос формируется путем объединения этих трёх частей в заданной последовательности.

3. **Генерация и запись ответа**  
   - Сформированный запрос отправляется выбранной модели Ollama.
   - Ответ модели выводится в консоль и дописывается в файл `04_model_response.txt`.
   - Скрипт обрабатывает сообщения последовательно, сохраняя номер последней обработанной строки для возможности продолжения с того же места в случае прерывания.

4. **Интерактивное управление**  
   - Пользователь может выбирать номер строки для начала генерации, что позволяет продолжать работу после прерываний.
   - При нажатии `CTRL+C` скрипт не завершается, а приостанавливается с сохранением последней обработанной строки и возвращается в главное меню.
   - В меню также предусмотрена опция перезапуска Ollama через отдельный скрипт `restart_ollama.py`.

## Применение скрипта

Скрипт может быть использован для:
- **Автоматизированной генерации контента:**  
  Помогает генерировать текстовые ответы, новости, статьи или сценарии, комбинируя ввод пользователя с предопределёнными подсказками и шаблонами.
  
- **Поддержки интерактивного диалога:**  
  Может быть применён в чат-ботах или системах автоматического ответа, где требуется динамически генерировать текстовые сообщения.

- **Обучения и прототипирования:**  
  Подходит для экспериментов с генеративными моделями, позволяя тестировать различные конфигурации и шаблоны ввода для получения разнообразных результатов.

- **Автоматизации рабочих процессов:**  
  Может интегрироваться в системы автоматизации для создания отчетов или других документов на основе шаблонов и пользовательских данных.

## Требования

- **Python 3.6+**
- Библиотека `ollama` (устанавливается через `pip install -r requirements.txt`)
- Стандартные библиотеки: `time`, `subprocess`, `os`, `glob`, `sys`, `signal`, `datetime`, `traceback`

## Установка и запуск

1. **Клонируйте репозиторий:**
   ```bash
   git clone <URL_вашего_репозитория>
   cd <папка_репозитория>
   ```

2. **Установите зависимости:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Запустите скрипт:**
   ```bash
   python <имя_скрипта>.py
   ```

При запуске скрипт проверит, что все необходимые переменные заданы. Если нет — он сразу предложит выбрать модель, файл с сообщениями и файл для записи ответов, после чего вы попадёте в главное меню для дальнейшей работы.

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности см. в файле [LICENSE](LICENSE).

## Контакты

Если у вас есть вопросы или предложения, создайте issue в репозитории или свяжитесь по [email](mailto:your.email@example.com).
```
