# Ollama-SSH: Генерация текстового контента

Этот скрипт предназначен для интерактивной генерации текстового контента с использованием моделей Ollama. Он объединяет выбор модели, файлов сообщений и файла для записи ответов в единый удобный интерфейс с гибким управлением процессом генерации.

## Описание

Скрипт выполняет следующие задачи:
- **Выбор конфигурации:**  
  При запуске, если переменные `MODEL_NAME`, `USER_MESSAGE_FILE` или `RESPONSE_FILE` не заданы, пользователь сразу перенаправляется на их выбор. Сначала выбирается модель, затем файл с сообщениями, после чего файл для записи ответов.
  
- **Интерактивное меню:**  
  Главное меню предоставляет следующие опции:
  - **1 - Начать генерацию:** Запуск процесса генерации сообщений построчно.  
  - **2 - Выбрать модель:** Перейти к выбору модели из списка доступных моделей Ollama.
  - **3 - Выбрать файлы:** Выбор файла с сообщениями и файла для записи ответов.
  - **4 - Перезапустить Ollama:** Запуск скрипта `restart_ollama.py` с параметром `--auto` в отдельном процессе для перезапуска сервиса.
  - **5 - Выход:** Завершение работы программы.
  
- **Гибкий выбор строки начала генерации:**  
  Перед началом генерации пользователь может указать номер строки, с которой начать обработку, либо выйти в меню, введя `"q"`. По умолчанию, если нажать Enter, генерация продолжается с последней обработанной строки.

- **Обработка прерываний (CTRL+C):**  
  Если во время генерации пользователь нажимает CTRL+C, скрипт приостанавливает выполнение, сохраняет номер последней обработанной строки и возвращается в главное меню.

- **Сохранение состояния:**  
  После обработки каждого сообщения переменная `LAST_PROCESSED_LINE` обновляется, что позволяет продолжать работу с того места, где она была прервана.

## Особенности

- **Автоматическая проверка начальных переменных:**  
  Функция `check_initial_variables()` проверяет при запуске, что все необходимые переменные установлены, и при необходимости предлагает их заполнить, не нарушая общую структуру кода.

- **Интерактивный пользовательский интерфейс:**  
  Меню позволяет пользователю контролировать процесс генерации, легко переключаться между настройками и управлять перезапуском сервиса Ollama.

- **Корректная обработка ошибок и прерываний:**  
  При возникновении ошибок (например, отсутствия доступных моделей или файлов) скрипт выводит понятные сообщения, а при нажатии CTRL+C — возвращается в главное меню, не завершая работу полностью.

## Требования

- **Python 3.6+**
- Библиотека `ollama` (для взаимодействия с сервером Ollama)
- Стандартные библиотеки: `time`, `subprocess`, `os`, `glob`, `sys`, `signal`, `datetime`, `traceback`

## Установка и запуск

1. **Клонируйте репозиторий:**
   ```bash
   git clone <URL_вашего_репозитория>
   cd <папка_репозитория>
   ```

2. **Установите зависимости (если требуется):**
   ```bash
   pip install ollama
   ```

3. **Запустите скрипт:**
   ```bash
   python <имя_скрипта>.py
   ```

При запуске, если какие-либо конфигурационные переменные не заданы, скрипт сразу предложит выбрать модель и файлы для сообщений и записи ответов. После этого вы попадёте в главное меню, где сможете управлять генерацией текстового контента.

## Пример главного меню

```
------------------------------------------------------------
Главное меню Генерации текстового контента
------------------------------------------------------------
Текущая конфигурация:
Модель: openchat:latest
Файл сообщений: 01_user_message.txt
Файл записи ответов: 04_model_response.txt
Строка начала обработки: 0
Последняя обработанная строка: 15
------------------------------------------------------------

1 - Начать генерацию
2 - Выбрать модель
3 - Выбрать файлы
4 - Перезапустить Ollama
5 - Выход
Выберите действие: _
```

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности см. в файле [LICENSE](LICENSE).

## Контакты

Если у вас есть вопросы или предложения, создайте issue в репозитории или свяжитесь по [email](mailto:your.email@example.com).
```
