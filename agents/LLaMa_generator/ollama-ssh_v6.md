
# Ollama-SSH: Генерация текстового контента

Этот скрипт предназначен для интерактивной генерации текстового контента с использованием моделей Ollama. Он объединяет выбор модели, файлов сообщений и файла для записи ответов в единый удобный интерфейс с гибким управлением процессом генерации.

## Как работает скрипт

Скрипт последовательно выполняет следующие шаги:


```plaintext
          +---------------------------+
          |       Запуск скрипта      |
          +---------------------------+
                      │
                      ▼
          +---------------------------+
          | Проверка переменных:      |
          | MODEL, USER_MESSAGE_FILE, |
          | RESPONSE_FILE             |
          +---------------------------+
                      │
              ┌───────┴─────────┐
              │                 │
              ▼                 ▼
+----------------------+  +----------------------+
| Если переменные не   |  | Если все переменные  |
| заполнены, то:       |  | заполнены            |
| - Выбор модели       |  +----------------------+
| - Выбор файла        |             │
|   сообщений          |             ▼
| - Выбор файла для    |  +---------------------------+
|   записи ответов     |  | Переход в главное меню:   |
+----------------------+  | вывод текущей конфигурации|
                          | (модель, файлы, номера     |
                          | строк) и опций меню        |
                          +---------------------------+
                                     │
                                     ▼
                         +------------------------------+
                         | Главное меню:                |
                         | 1 - Начать генерацию         |
                         | 2 - Выбрать модель           |
                         | 3 - Выбрать файлы            |
                         | 4 - Перезапустить Ollama     |
                         | 5 - Выход                    |
                         +------------------------------+
                                     │
                     ┌───────────────┴───────────────┐
                     │                               │
                     ▼                               ▼
          +-------------------+           +-------------------+
          | 1 - Начать        |           | 2/3 - Выбор       |
          | генерацию         |           | модели/файлов     |
          +-------------------+           +-------------------+
                     │                               │
                     ▼                               ▼
          +---------------------------+    (Обновление переменных)
          | Ввод номера строки для    |
          | начала генерации (или "q"  |
          | для возврата в меню)      |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Чтение файла 01_user_     |
          | message.txt построчно     |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Формирование запроса:     |
          | 1. Добавление содержимого |
          |    02_sys_prompt.txt      |
          | 2. Добавление содержимого |
          |    03_response_template.txt|
          | 3. Добавление строки из    |
          |    01_user_message.txt    |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Отправка запроса в модель |
          | Ollama и получение ответа |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Запись ответа в файл      |
          | 04_model_response.txt      |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Обновление LAST_PROCESSED  |
          | _LINE после каждой строки  |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Обработка прерывания (CTRL+C)|
          | - Сохранение состояния     |
          | - Возврат в главное меню   |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          | Опция перезапуска Ollama:  |
          | Запуск restart_ollama.py   |
          +---------------------------+
                     │
                     ▼
          +---------------------------+
          |       Выход из скрипта    |
          +---------------------------+
```


1. **Выбор конфигурации**  
   При запуске, если конфигурационные переменные не заданы, скрипт сразу предлагает выбрать:
   - Модель для генерации (через вызов API Ollama для получения списка доступных моделей).
   - Файл с сообщениями (например, `01_user_message.txt`).
   - Файл для записи ответов (например, `04_model_response.txt`).

2. **Формирование запроса для генерации**  
   Для каждой строки из файла `01_user_message.txt` скрипт:
   - Считывает строку сообщения.
   - Добавляет содержимое файла `02_sys_prompt.txt` (системное задание).
   - Добавляет содержимое файла `03_response_template.txt` (шаблон ответа).
   
   Таким образом, окончательный запрос формируется путем объединения этих трёх частей в заданной последовательности.

3. **Генерация и запись ответа**  
   - Сформированный запрос отправляется выбранной модели Ollama.
   - Ответ модели выводится в консоль и дописывается в файл `04_model_response.txt`.
   - Скрипт обрабатывает сообщения последовательно, сохраняя номер последней обработанной строки для возможности продолжения с того же места в случае прерывания.

4. **Интерактивное управление**  
   - Пользователь может выбирать номер строки для начала генерации, что позволяет продолжать работу после прерываний.
   - При нажатии `CTRL+C` скрипт не завершается, а приостанавливается с сохранением последней обработанной строки и возвращается в главное меню.
   - В меню также предусмотрена опция перезапуска Ollama через отдельный скрипт `restart_ollama.py`.

## Применение скрипта

Скрипт может быть использован для:
- **Автоматизированной генерации контента:**  
  Помогает генерировать текстовые ответы, новости, статьи или сценарии, комбинируя ввод пользователя с предопределёнными подсказками и шаблонами.
  
- **Поддержки интерактивного диалога:**  
  Может быть применён в чат-ботах или системах автоматического ответа, где требуется динамически генерировать текстовые сообщения.

- **Обучения и прототипирования:**  
  Подходит для экспериментов с генеративными моделями, позволяя тестировать различные конфигурации и шаблоны ввода для получения разнообразных результатов.

- **Автоматизации рабочих процессов:**  
  Может интегрироваться в системы автоматизации для создания отчетов или других документов на основе шаблонов и пользовательских данных.


```plaintext
                     +---------------------------------------+
                     |        Входящие заказы (почта)        |
                     |       01_user_message.txt             |
                     +----------------+----------------------+
                                      │
                                      ▼
                     +---------------------------------------+
                     |  Системный промпт для формирования     |
                     |         ответа (02_sys_prompt.txt)      |
                     +----------------+----------------------+
                                      │
                                      ▼
                     +---------------------------------------+
                     |      Шаблон ответа (03_response_template.txt)     
                     +----------------+----------------------+
                                      │
                                      ▼
                     +---------------------------------------+
                     | Генерация ответа через модель Ollama   |
                     +----------------+----------------------+
                                      │
                                      ▼
                     +---------------------------------------+
                     | Запись структурированного ответа с   |
                     | уникальным номером в файл              |
                     | 04_model_response.txt                  |
                     +----------------+----------------------+
                                      │
                                      ▼
                     +---------------------------------------+
                     |  Обработка файлом маршрутеризатором     |
                     | (определение дальнейшей маршрутизации)  |
                     +---------------------------------------+
```

### Пример использования: Обработка почты от пользователей (заказы пиццерии)

1. **Входящие заказы (файл 01_user_message.txt):**  
   Допустим, другой бот получает письма с заказами от клиентов и записывает их построчно в файл. Примеры заказов:
   - **Заказ 1:**  
     ```
     Здравствуйте, я хотел бы заказать пиццу "Маргарита" с дополнительным сыром, апельсиновый сок и свежий салат. Прошу связаться для уточнения деталей.
     ```
   - **Заказ 2:**  
     ```
     Добрый день, прошу оформить заказ: пицца "Пепперони", напиток "Кола", салат "Цезарь". Благодарю!
     ```
   - **Заказ 3:**  
     ```
     Привет, хочу заказать большой набор: пицца "Четыре сыра", морс и салат "Греческий". Жду подтверждения заказа.
     ```

2. **Системный промпт (файл 02_sys_prompt.txt):**  
   Содержит инструкции для бота, как обрабатывать заказы. Промпт может включать информацию о том, как структурировать ответ, проверить наличие всех позиций и указать контактные данные.

3. **Шаблон ответа (файл 03_response_template.txt):**  
   Определяет формат, структуру и поля ответа, например:
   - Номер заказа
   - Перечень позиций
   - Сроки выполнения
   - Информация о дополнительной обработке (например, если заказ требует отдельного внимания отдела заказов)

4. **Генерация и запись ответа:**  
   Скрипт объединяет:
   - Заказ из файла 01_user_message.txt
   - Системный промпт из 02_sys_prompt.txt
   - Шаблон ответа из 03_response_template.txt  
   
   Получив итоговый запрос, бот отправляет его в модель Ollama, получает сформированный ответ, присваивает ему уникальный номер и записывает результат в файл 04_model_response.txt.

5. **Маршрутизация:**  
   После записи, другой бот-маршрутизатор обрабатывает файл 04_model_response.txt. В зависимости от номера маршрута в ответе:
   - Ответ может быть отправлен обратно пользователю (если заказ подтверждён).
   - Либо заказ перенаправляется в отдел заказов для дальнейшей обработки.

Таким образом, автоматизированный процесс позволяет эффективно обрабатывать большое количество заказов, структурировать и маршрутизировать ответы, избавляя операторов от рутинной работы по индивидуальному ответу на каждое письмо.

### Описание

1. **Входящие заказы (01_user_message.txt):**  
   Здесь поступают заказы от пользователей, например:  
   > "Здравствуйте, я хотел бы заказать пиццу 'Маргарита' с дополнительным сыром, апельсиновый сок и свежий салат."

2. **Системный промпт (02_sys_prompt.txt):**  
   Содержит инструкции для генерации ответа, например:  
   > "Форматируйте ответ, указывайте номер заказа, перечень позиций и контактные данные."

3. **Шаблон ответа (03_response_template.txt):**  
   Определяет структуру ответа, например:  
   > "Номер заказа, список позиций, сроки выполнения"

4. **Генерация ответа (Ollama):**  
   Скрипт объединяет содержимое всех трёх файлов и отправляет запрос в модель Ollama для генерации ответа.

5. **Запись ответа (04_model_response.txt):**  
   Сгенерированный ответ записывается с уникальным номером, соответствующим порядковому номеру заказа.

6. **Маршрутизация:**  
   После записи, другой бот-маршрутизатор анализирует файл и определяет, кому передать ответ — напрямую пользователю или в отдел заказов для дальнейшей обработки.

Эта схема иллюстрирует автоматизированный процесс обработки заказов, позволяющий эффективно управлять потоком входящих сообщений и генерировать структурированные ответы без ручного вмешательства.

## Требования

- **Python 3.6+**
- Библиотека `ollama` (устанавливается через `pip install -r requirements.txt`)
- Стандартные библиотеки: `time`, `subprocess`, `os`, `glob`, `sys`, `signal`, `datetime`, `traceback`

## Установка и запуск

1. **Клонируйте репозиторий:**
   ```bash
   git clone <URL_вашего_репозитория>
   cd <папка_репозитория>
   ```

2. **Установите зависимости:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Запустите скрипт:**
   ```bash
   python <имя_скрипта>.py
   ```

При запуске скрипт проверит, что все необходимые переменные заданы. Если нет — он сразу предложит выбрать модель, файл с сообщениями и файл для записи ответов, после чего вы попадёте в главное меню для дальнейшей работы.

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности см. в файле [LICENSE](LICENSE).

## Контакты

Если у вас есть вопросы или предложения, создайте issue в репозитории или свяжитесь по [email](mailto:your.email@example.com).
```
