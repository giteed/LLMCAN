# Ollama-SSH: Генерация текстового контента

Интерактивный скрипт для генерации текстового контента с использованием моделей Ollama. Он объединяет выбор модели, файлов сообщений и файла для записи ответов в единый удобный интерфейс, обеспечивая гибкое управление процессом генерации.

---

## Содержание

- [Описание](#описание)
- [Как работает скрипт](#как-работает-скрипт)
  - [Общая схема работы](#общая-схема-работы)
  - [Пример использования: Обработка заказов по почте](#пример-использования-обработка-заказов-по-почте)
- [Требования](#требования)
- [Установка и запуск](#установка-и-запуск)
- [Лицензия](#лицензия)
- [Контакты](#контакты)

---

## Описание

Скрипт **Ollama-SSH** предназначен для автоматизированной генерации текстового контента. Он:
- **Выбирает конфигурацию** – модель, файл с сообщениями и файл для записи ответов.
- **Формирует запрос** для генерации, объединяя содержимое трёх файлов:
  - `01_user_message.txt` – исходные сообщения или заказы,
  - `02_sys_prompt.txt` – системные инструкции,
  - `03_response_template.txt` – шаблон структуры ответа.
- **Отправляет запрос** выбранной модели Ollama и получает сгенерированный ответ.
- **Записывает ответ** в файл `04_model_response.txt`, сохраняя уникальный номер и номер маршрута.
- **Обеспечивает интерактивное управление** – возможность выбора строки для начала генерации, обработку прерываний (CTRL+C) и перезапуск Ollama.

---

## Как работает скрипт

### Общая схема работы

Скрипт выполняет следующие шаги:

1. **Выбор конфигурации:**  
   При запуске, если переменные не заданы, скрипт предлагает выбрать:
   - Модель (через API Ollama),
   - Файл с сообщениями (например, `01_user_message.txt`),
   - Файл для записи ответов (например, `04_model_response.txt`).

2. **Формирование запроса:**  
   Для каждой строки из `01_user_message.txt`:
   - Считывается сообщение,
   - Добавляется содержимое `02_sys_prompt.txt` (инструкция),
   - Добавляется содержимое `03_response_template.txt` (шаблон ответа).

3. **Генерация и запись ответа:**  
   - Сформированный запрос отправляется модели Ollama,
   - Полученный ответ выводится в консоль и записывается в `04_model_response.txt`,
   - Обновляется переменная последней обработанной строки для возможности продолжения обработки после прерывания.

4. **Интерактивное управление:**  
   - Пользователь может выбрать номер строки для начала генерации,
   - При нажатии `CTRL+C` происходит приостановка работы с сохранением состояния и возврат в главное меню,
   - Предусмотрена опция перезапуска Ollama через отдельный скрипт `restart_ollama.py`.


#### Общая диаграмма работы

![Описание картинки](https://raw.githubusercontent.com/giteed/LLMCAN/main/agents/LLaMa_generator/order_processing_diagram.png)

Эта диаграмма демонстрирует замкнутый цикл обработки:
- **Список заказов (01_user_message.txt):** Заказы поступают постоянно.
- **Системный промпт (02_sys_prompt.txt)** и **шаблон ответа (03_response_template.txt):** Формируют структуру запроса.
- **Генерация ответа (Ollama)** и **запись в файл (04_model_response.txt):** Ответ сохраняется с уникальным номером и информацией о маршрутизации.
- **Маршрутизация:** Определяется дальнейшая обработка ответа (например, отправка обратно пользователю или передача в отдел заказов).
- После маршрутизации система возвращается к обработке новых заказов.

### Пример использования: Обработка заказов по почте

Представьте, что система применяется для автоматической обработки заказов пиццерии, где входящие заказы поступают по электронной почте и записываются в файл `01_user_message.txt`. Примеры заказов:

- **Заказ 1:**
  ```
  Здравствуйте, я хотел бы заказать пиццу "Маргарита" с дополнительным сыром, апельсиновый сок и свежий салат. Прошу связаться для уточнения деталей.
  ```
- **Заказ 2:**
  ```
  Добрый день, прошу оформить заказ: пицца "Пепперони", напиток "Кола", салат "Цезарь". Благодарю!
  ```
- **Заказ 3:**
  ```
  Привет, хочу заказать большой набор: пицца "Четыре сыра", морс и салат "Греческий". Жду подтверждения заказа.
  ```

**Процесс обработки заказа:**
1. **Входящий заказ** из файла `01_user_message.txt` объединяется с содержимым файлов `02_sys_prompt.txt` и `03_response_template.txt`.
2. **Генерация ответа:** Модель Ollama формирует структурированный ответ, включающий номер заказа, перечень позиций, сроки выполнения и дополнительную информацию.
3. **Запись ответа:** Ответ сохраняется в файл `04_model_response.txt` с уникальным номером.
4. **Маршрутизация:** В зависимости от номера маршрута, ответ либо отправляется непосредственно клиенту, либо передается в отдел заказов для дальнейшей обработки.

Таким образом, автоматизация процесса позволяет оперативно обрабатывать большое количество заказов и избавляет операторов от ручного ввода ответов.

---

## Требования

- **Python 3.6+**
- Библиотека [ollama](https://pypi.org/project/ollama/)  
  (Устанавливается командой: `pip install -r requirements.txt`)
- Стандартные библиотеки Python: `time`, `subprocess`, `os`, `glob`, `sys`, `signal`, `datetime`, `traceback`

---

## Установка и запуск

1. **Клонируйте репозиторий:**
   ```bash
   git clone <URL_вашего_репозитория>
   cd <папка_репозитория>
   ```

2. **Установите зависимости:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Запустите скрипт:**
   ```bash
   python <имя_скрипта>.py
   ```
   
При запуске скрипт проверит, что все необходимые переменные заданы. Если нет — он сразу предложит выбрать модель, файл с сообщениями и файл для записи ответов, после чего вы попадёте в главное меню для дальнейшей работы.

---

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности см. в файле [LICENSE](LICENSE).

---

## Контакты

Если у вас есть вопросы или предложения, создайте issue в репозитории или свяжитесь по [email](mailto:your.email@example.com).
```

